### 190314
* ì¸ê³µì§€ëŠ¥ì„ ìœ„í•œ ì‘ìš©ìˆ˜í•™ : ì„ í˜•ëŒ€ìˆ˜, í™•ë¥ í†µê³„, ìˆ˜ì¹˜í•´ì„ [[link]](https://www.deeplearningbook.org/)
* ì¸ê³µì§€ëŠ¥ ë¶„ì•¼ì—ì„œëŠ” ìƒí™©ì„ íŒë‹¨í•˜ëŠ” í•˜ë‚˜ì˜ ë°©ë²•ìœ¼ë¡œ ì •ë‹µì´ ë  í™•ë¥ ì´ ê°€ì¥ ë†’ì€ ê²ƒì„ ì •ë‹µìœ¼ë¡œ ì±„íƒí•˜ëŠ” ë°©ë²•ì„ ìì£¼ ì‚¬ìš©í•œë‹¤.

* í™•ë¥ 

$$
í™•ë¥  \ = \frac{ì–´ë–¤\ ì‚¬ê±´ì´\ ë°œìƒí• \ ìˆ˜\ ìˆëŠ”\ ê²½ìš°ì˜\ ê°€ì§“\ ìˆ˜}{ëª¨ë“  \ ê²½ìš°ì˜\ ê°€ì§“ìˆ˜}
$$

```
ì˜ˆ) ì£¼ì‚¬ìœ„ì˜ 1ì´ ë‚˜ì˜¬ ê²½ìš°ì˜ ìˆ˜ë¥¼ êµ¬í•˜ì‹œì˜¤.
ì£¼ì‚¬ìœ„ë¥¼ ë˜ì ¸ ë‚˜ì˜¤ëŠ” ëª¨ë“  ê²½ìš°ì˜ ìˆ˜ : 1,2,3,4,5,6 ì´ 6ê°€ì§€
ì£¼ì‚¬ìœ„ 1ì´ ë‚˜ì˜¤ ê²½ìš°ì˜ ìˆ˜ : 1ê°€ì§€
ì£¼ì‚¬ìœ„ë¥¼ ë˜ì¡Œì„ ë•Œ 1ì´ ë‚˜ì˜¬ í™•ë¥  = 1/6
```

* í™•ë¥ ë³€ìˆ˜(Random Variable)
  * ì •ì˜ì—­ì´ í‘œë³¸ê³µê°„, ê³µì—­ì´ ì‹¤ìˆ˜ ì „ì²´ì˜ ì§‘í•©ì¸ 'í•¨ìˆ˜'
  * ê·¸ëŸ¬ë‚˜ ë³€ìˆ˜ì˜ ì—­í• ì„ í•˜ë¯€ë¡œ í™•ë¥  ë³€ìˆ˜ë¼ê³  í•œë‹¤.
  * í•œ ê°œì˜ ë™ì „ì„ ë‘ ë²ˆ ë˜ì§€ëŠ” ì‹œí–‰ì—ì„œ ì•ë©´ì´ ë‚˜ì˜¤ëŠ” íšŸìˆ˜ë¥¼ í™•ë¥ ë³€ìˆ˜ $$X$$ ë¼ê³  í•˜ì. 
  * $$X=f(S)$$ $$x$$ ëŠ” ë™ì „ ë‘ ê°œë¥¼ ë˜ì ¸ ì•ë©´ì´ ë‚˜ì˜¤ëŠ” ëª¨ë“  ê²½ìš°ì˜ ìˆ˜ $$(S_i)$$ ë¥¼ êµ¬í•˜ëŠ” 'í•¨ìˆ˜' ì´ë‹¤.
* ì´ì‚°í™•ë¥ ë³€ìˆ˜(Discrete Random Variable)
  * í™•ë¥ ë³€ìˆ˜ Xê°€ ê°€ì§€ëŠ” ê°’ì´ ìœ í•œê°œì´ê±°ë‚˜ ìì—°ìˆ˜ì™€ ê°™ì´ ì…€ ìˆ˜ ìˆì„ ë•Œ, Xë¥¼ ì´ì‚°í™•ë¥ ë³€ìˆ˜ë¼ê³  í•œë‹¤.
* ì—°ì†í™•ë¥ ë³€ìˆ˜(Continuous Random Variable)
  * í™•ë¥ ë³€ìˆ˜ Xê°€ ê°€ì§€ëŠ” ê°’ì´ ë¬´í•œê°œì—¬ì„œ ì…€ ìˆ˜ ì—†ì„ ë•Œ, Xë¥¼ ì—°ì†í™•ë¥ ë³€ìˆ˜ë¼ê³  í•œë‹¤.
  * ex) í‚¤, ëª¸ë¬´ê²Œ, ë„“ì´
  * ì–´ë–¤ ì‚¬ê±´ì˜ ì—°ì†í™•ë¥ ë³€ìˆ˜ê°€ xì¼ ë•Œ, ê·¸ì— ëŒ€í•œ í™•ë¥  PëŠ” ì—°ì†í™•ë¥ ë¶„í¬ $$f(x)$$ ë¥¼ ì§€ì •í•œ $$X$$ ì˜ êµ¬ê°„ ì•ˆì—ì„œ ì ë¶„í•œ ê²ƒê³¼ ê°™ë‹¤. 

$$
P(a\le X\le b) = \int_{a}^{b} f(x)\, dx
$$



* í™•ë¥  ë¶„í¬(Probability Distribution)

  * ì •ì˜ì—­ì´ í™•ë¥  ë³€ìˆ˜, ê³µì—­ì´ í™•ë¥ ì˜ ì§‘í•©ì¸ 'í•¨ìˆ˜'
  * ì•ë©´ì„ H, ë’·ë©´ì„ Të¼ í•˜ë©´ í‘œë³¸ ê³µê°„ SëŠ” S={HH, HT, TH, TT} ì´ë¯€ë¡œ, ì´ì‚°í™•ë¥ ë³€ìˆ˜ Xê°€ ì·¨í•  ìˆ˜ ìˆëŠ” ê°’ì€ 0,1,2 ì´ê³ , ê° ê°’ì„ ì·¨í•  í™•ë¥ ì€ 1/4, 1/2, 1/4 ì´ë‹¤.
  * ì¦‰, P(X=0) = 1/4, P(X=1) = 1/2, P(X=2) = 1/4
  * $$P=g(X)$$ $$P$$ ëŠ” ë™ì „ ë‘ ê°œ ì¤‘ ì•ë©´ì´ $$x_i$$ ê°œ ë‚˜ì˜¬ í™•ë¥  ë¶„í¬ í•¨ìˆ˜ì´ë‹¤.
  * Histogram í‘œí˜„ := í™•ë¥ ë¶„í¬í‘œ

* í™•ë¥ í•¨ìˆ˜

  * í™•ë¥  ì§ˆëŸ‰ í•¨ìˆ˜(Probability Mass Function)
    * ì´ì‚° í™•ë¥  ë¶„í¬ì˜ í™•ë¥  í•¨ìˆ˜
  * í™•ë¥  ë°€ë„ í•¨ìˆ˜(Probability Density Function)
    * ì—°ì† í™•ë¥  ë¶„í¬ì˜ í™•ë¥  í•¨ìˆ˜

* ê²°í•©í™•ë¥ ê³¼ ì¡°ê±´ë¶€í™•ë¥  (Joint Probability & Conditional Probability)

  * ê²°í•©í™•ë¥ ì˜ ì •ì˜
    * ì‚¬ê±´ Aì™€ ì‚¬ê±´ Bê°€ ì„œë¡œ ë…ë¦½ëœ ì‚¬ê±´ì¼ ë•Œ, ë‘ ì‚¬ê±´ì˜ ê²°í•© í™•ë¥ ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

  $$
  P(A\cap B)=P(A,B)=P(A)P(B)
  $$

  

  * ì¡°ê±´ë¶€í™•ë¥ ì˜ ì •ì˜
    * ì‚¬ê±´ Bê°€ ì¼ì–´ë‚¬ì„ ë•Œ, ì‚¬ê±´ Aê°€ ì¼ì–´ë‚  ì¡°ê±´ë¶€ í™•ë¥ ì„ ë‹¤ìŒê³¼ ê°™ë‹¤.

  $$
  P(A|B)=\frac{P(A\cap B)}{P(B)}=\frac{P(A,B)}{P(B)}
  $$

  * ë¬¸ì œ 1.

  ```
  í•˜ë‚˜ì˜ ì£¼ì‚¬ìœ„ë¥¼ ë‘ ë²ˆ ë˜ì§„ë‹¤ê³  ê°€ì •í•˜ì.
  ì‚¬ê±´ AëŠ” ì£¼ì‚¬ìœ„ë¥¼ ë‘ë²ˆ ë˜ì ¸ì„œ ë‚˜ì˜¨ ìˆ«ìì˜ í•©ì´ 8ì´ìƒì¼ ì‚¬ê±´ì´ê³ , ì‚¬ê±´ BëŠ” ì²«ë²ˆì§¸ ì£¼ì‚¬ìœ„ë¥¼ ë˜ì¡Œì„ ë•Œ ë‚˜ì˜¤ëŠ” ìˆ«ìê°€ 5ê°€ ë˜ëŠ” ì‚¬ê±´ ì´ë¼ê³  í• ë•Œ, ê²°í•©í™•ë¥ P(A,B)ì™€ ì¡°ê±´ë¶€í™•ë¥  P(A|B)ë¥¼ êµ¬í•˜ì‹œì˜¤.
  ```

  * ì •ë‹µ

  ```
  ê²°í•©í™•ë¥  P(A,B)ë¥¼ êµ¬í•´ë³´ì. P(A,B)ëŠ” ì²« ì£¼ì‚¬ìœ„ë¥¼ ë˜ì¡Œì„ ë•Œ 5ê°€ ë‚˜ì˜¤ê³ , ë‘ ë²ˆì§¸ ì£¼ì‚¬ìœ„ë¥¼ ë˜ì¡Œì„ ë•Œ, 3ì´ìƒì˜ ìˆ˜ê°€ ë‚˜ì˜¬ í™•ë¥ ì„ ì˜ë¯¸í•œë‹¤. 1/6 * 4/6 = 1/9 ê°€ ëœë‹¤. ë‹¤ìŒì€ ì¡°ê±´ë¶€ í™•ë¥  P(A|B)ë¥¼ êµ¬í•´ë³´ì. P(B)ëŠ” ì²«ë²ˆì§¸ ì£¼ì‚¬ìœ„ë¥¼ ë˜ì¡Œì„ ë•Œ, 5ê°€ ë‚˜ì˜¤ëŠ” ì‚¬ê±´ì´ë¯€ë¡œ í™•ë¥ ì€ 1/6ì´ ëœë‹¤. ì¡°ê±´ë¶€ í™•ë¥  ì •ì˜ P(A|B) = P(A,B)/P(B) ì— ë”°ë¥´ë©´, 1/9 / 1/6 = 2/3 ì´ ë˜ì–´ P(A|B)ì˜ í™•ë¥ ì€ 2/3ì´ ëœë‹¤. 
  ```

  * ë¬¸ì œ2. 

  ```
  ë°±ë§Œ ëª…ì— ë‹¤ì„¯ ëª… ê¼´ë¡œ ì–´ë–¤ ì§ˆë³‘ì— ê±¸ë¦°ë‹¤ê³  ê°€ì •í•˜ì. ì´ ì§ˆë³‘ì„ ì§„ë‹¨í•˜ëŠ”ë° ìµœì‹  AI ê¸°ìˆ ì„ ì‚¬ìš©í•˜ë©´, 99.99%ì˜ ì •ë°€ë„ë¡œ ê·¸ ì‚¬ëŒì´ ì§ˆë³‘ì— ê±¸ë ¸ëŠ”ì§€ ì•ˆ ê±¸ë ¸ëŠ”ì§€ë¥¼ íŒì •í•  ìˆ˜ ìˆë‹¤. (0.01%í™•ë¥ ë¡œ ì˜ëª»ëœ íŒì •ì„ í•  ìˆ˜ ìˆìŒ) ìµœìˆ˜ì •ì”¨ëŠ” ì‹œí—˜ì‚¼ì•„ ìµœì‹  AI ê²€ì‚¬ë¥¼ ë°›ì•˜ë‹¤. ì§„ë‹¨ ê²°ê³¼ê°€ ì–‘ì„±ìœ¼ë¡œ ë‚˜ì™”ë‹¤ë©´ ì‹¤ì œë¡œ ìµœìˆ˜ì •ì”¨ê°€ ì´ ì§ˆë³‘ì— ê±¸ë ¸ì„ í™•ë¥ ì€ ì–¼ë§ˆì¼ê¹Œìš”?
  
  ```

  * ì •ë‹µ(ë§ë‚˜?)

  ```
  ì‚¬ê±´ A: ì§ˆë³‘ì— ê±¸ë¦´ ì‚¬ê±´
  ì‚¬ê±´ B: ì§„ë‹¨ ê²°ê³¼ê°€ ì–‘ì„±ìœ¼ë¡œ ë‚˜ì˜¬ ì‚¬ê±´
  P(A,B) = P(A)P(B) = 0.00005 * 0.9999
  P(A|B) = P(A,B)/P(B) = 0.00005 * 0.9999 / 0.9999 = 0.00005
  ```

* ì‚¬ìŠ¬ë²•ì¹™(Chain rule)

  * ì¡°ê±´ë¶€í™•ë¥ ê³¼ ê²°í•©í™•ë¥ ì˜ ê´€ê³„ë¥¼ í™•ì¥í•˜ë©´ ë³µìˆ˜ì˜ ì‚¬ê±´ X1,X2,...,XN ì— ëŒ€í•œ ì¡°ê±´ë¶€ í™•ë¥ ì„ ë‹¤ìŒì²˜ëŸ¼ ì“¸ ìˆ˜ìˆë‹¤. ì´ë¥¼ ì‚¬ìŠ¬ë²•ì¹™(chain rule)ì´ë¼ê³  í•œë‹¤.

  $$
  P(X_1,X_2) = P(X_1)P(X_2|X_1) \\
  P(X_1,X_2,X_3) = P(X_3|X_1,X_2)P(X_1,X_2) \\
  =P(X_1)P(X_2|X_1)P(X_3|X_1,X_2) \\
  P(X_1,X_2,X_3,X_4) = P(X_4|X_1,X_2,X_3)P(X_1,X_2,X_3) \\
  =P(X_1)P(X_2|X_1)P(X_3|X_1,X_2)P(X_4|X_1,X_2,X_3)
  $$



* ê¸°ëŒ“ê°’(Expectation Value)

  * ë‚˜ì˜¬ ê²ƒì´ë¼ê³  ì˜ˆìƒí•˜ëŠ” ê°’
  * Xê°€ í™•ë¥ ë³€ìˆ˜ì´ê³  í™•ë¥  P(X)ì¸ ì‚¬ê±´ì´ ë²Œì–´ì§ˆ ë•Œ, ì˜ˆìƒí•  ìˆ˜ ìˆëŠ” ê²°ê´ê°’ì´ ê¸°ëŒ€ ê°’ì„
  * ëª¨ë“  ì´ì‚°í™•ë¥ ë³€ìˆ˜ Xì— ëŒ€í•œ ê¸°ëŒ“ ê°’ E(X)ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. (ì´ ë•Œ, í™•ë¥ ì€ P(X))

  $$
  E(X) = \sum P(X) \cdot X
  $$

* ê¸°ëŒ“ê°’ì˜ ì†ì„±(Properties of Expectation Value)

  * $$E(k) = k $$ (ìƒìˆ˜ì˜ ê¸°ëŒ“ê°’ì€ ìƒìˆ˜ê°€ ëœë‹¤.)
  * $$E(kX) = kE(X)$$ (í™•ë¥ ë³€ìˆ˜ë¥¼ ìƒìˆ˜ ë°°í•˜ë©´, ê¸°ëŒ“ê°’ë„ ìƒìˆ˜ ë°°ê°€ ëœë‹¤)
  * $$E(X+Y) = E(X) + E(Y)$$ (í™•ë¥ ë³€ìˆ˜ì˜ í•©ì˜ ê¸°ëŒ“ê°’ì€ ê° ê¸°ëŒ“ê°’ì˜ í•©ê³¼ ê°™ë‹¤)
  * $$X$$ ì™€ $$Y$$ ê°€ ì„œë¡œ ë…ë¦½ì¼ ë•Œ $$E(XY) = E(X)E(Y)$$ (ë…ë¦½ì ì¸ í™•ë¥ ë³€ìˆ˜ì˜ ê³±ì— ëŒ€í•œ ê¸°ëŒ“ê°’ì€ ê° ê¸°ëŒ“ê°’ì˜ ê³±ê³¼ ê°™ë‹¤)

```
ì˜ˆì‹œ) 1ë“± ìƒê¸ˆì´ 1ì–µì›ì¸ ë³µê¶Œì´ 1ì¥, 2ë“± ìƒê¸ˆì´ 100ë§Œì›ì´ ë³µê¶Œì´ 10ì¥, 3ë“± ìƒê¸ˆì´ 1ë§Œì›ì´ ë³µê¶Œì´ 1000ì¥ ìˆë‹¤ê³  ê°€ì •í•©ì‹œë‹¤. ì´ ë³µê¶Œì˜ ì´ íŒë§¤ëŸ‰ì€ ë°±ë§Œì¥ì…ë‹ˆë‹¤. ì´ ë³µê¶Œì„ 1ì¥ ìƒ€ì„ ë•Œ ê¸°ëŒ€í•  ìˆ˜ ìˆëŠ” ë‹¹ì²¨ ê¸ˆì•¡ì€ ì–¼ë§ˆì¼ê¹Œìš”?
```

```
 Answer)
ì´ë•Œì˜ í™•ë¥  ë³€ìˆ˜ë¥¼ X, ê·¸ì— ëŒ€í•œ í™•ë¥ ì„ P(X)ë¼ê³  í• ë•Œ, ì´ì‚°í™•ë¥ ë¶„í¬ëŠ” ë‹¤ìŒ í‘œì™€ ê°™ìŠµë‹ˆë‹¤.
ê¸°ëŒ“ê°’ EëŠ” ëª¨ë“  í™•ë¥ ë³€ìˆ˜ Xì™€ ê·¸ì— ëŒ€í•œ ë°œìƒí™•ë¥  P(X)ë¥¼ ê³±í•œ ë‹¤ìŒ, ëª¨ë‘ë¥¼ ë”í•œ ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ë‹¤ìŒê³¼ ê°™ì´ í’€ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ğ¸ = (100,000,000*1)/1,000,000 + (1,000,000*10)/1,000,000 + (10,000*1000)/1,000,000 + 0
= 100+10+10 = 120
ì—¬ê¸°ì„œ ë§í•˜ëŠ” ê¸°ëŒ“ê°’ E=120ì˜ ì˜ë¯¸ëŠ” ë³µê¶Œ 1ì¥ì„ ìƒ€ì„ ë•Œ ì˜ˆìƒë˜ëŠ” ë‹¹ì²¨ ê¸ˆì•¡ì´ 120ì›ì´ë¼ëŠ” ëœ»ì…ë‹ˆë‹¤.
```



* Mean :=Expectation Value

  * í‰ê· ì˜ ì •ì˜(Definition of Mean)

    * í‰ê· ì€ ìˆ˜í•™ì ìœ¼ë¡œ ê¸°ëŒ“ê°’ê³¼ ê°™ì€ ì˜ë¯¸ì´ë‹¤.
    * ì¦‰, 'ê³¼ê±° 6ê°œì›” ê°„ì˜ ë§¤ì¶œ í‰ê· ì´ ë‹¤ìŒë‹¬ì˜ ì˜ˆìƒ ë§¤ì¶œì•¡ì´ ëœë‹¤'ëŠ” í™•ë¥ ì˜ ê´€ì ì—ì„œ ë‹¬ë¦¬ í‘œí˜„í•˜ìë©´ '6ê°œì˜ í™•ë¥ ë³€ìˆ˜ê°€ ê°ê° ê°™ì€ í™•ë¥ (1/6)ë¡œ ë°œìƒí•˜ë¯€ë¡œ ë‹¤ìŒ í•œë‹¬ ë™ì•ˆì˜ ë§¤ì¶œì— ëŒ€í•œ ê¸°ëŒ“ê°’ì€ ê° ì›”ì˜ ë§¤ì¶œì— 1/6ì„ ê³±í•œ ê²ƒì„ ëª¨ë‘ ë”í•œ í•©ê³„ì™€ ê°™ë‹¤'ë¼ê³  í•˜ëŠ” ê²ƒ

    $$
    Nê°œì˜\ í™•ë¥ ë³€ìˆ˜ê°€\ ê°ê° x_1,x_2,x_3,...,x_nì´ë¼ëŠ”\ ê°’ì„\ ê°€ì§ˆ\ ë•Œ\ í‰ê· \ ê°’ì€\ ë‹¤ìŒê³¼\ ê°™ë‹¤. \\
    \bar{x} = \sum_{k=1}^n\frac{1}{n}\cdot x_k = \frac{1}{n}\sum_{k=1}^n x_k
    $$

```
Question) ê·¸ëŸ°ë° ê³¼ì—° ì•ì˜ ë°©ì‹ëŒ€ë¡œ í‰ê· ê°’ë§Œ êµ¬í•˜ë©´ ë‹¤ìŒ ë‹¬ì˜ ë§¤ì¶œì„ ì˜¬ë°”ë¥´ê²Œ ì˜ˆìƒí•  ìˆ˜ ìˆëŠ” ê²ƒì¸ê°€?
```

```
Question) ê·¸ë˜ì„œ ë‹¨ìˆœíˆ í‰ê· ê°’ì„ êµ¬í•˜ëŠ” ë°©ë²• ì™¸ì—ë„ í‰ê· ê°’ê³¼ ì‹¤ ë°ì´í„°ê°€ ì–¼ë§ˆë‚˜ ì°¨ì´(Deviation, í¸ì°¨)ê°€ ë‚˜ëŠ”ì§€ì— ëŒ€í•´ ìƒê°í•´ ë´…ì‹œë‹¤.
```



* í¸ì°¨ vs ë¶„ì‚°(Deviation vs Variance)

  * í¸ì°¨ì˜ ë¬¸ì œ
    * ë§¤ë‹¬ ì–¼ë§ˆë§Œí¼ì˜ ë§¤ì¶œì•¡ì´ ê³ ê°ë³„ë¡œ í©ì–´ì ¸ ìˆëŠ”ì§€ ë¶„ì„í–ˆìœ¼ë‚˜, í¸ì°¨ì˜ í•©ê³„ë¥¼ êµ¬í•´ë³´ë©´ 0ì´ ë˜ì–´ë²„ë¦°ë‹¤.
    * (+) ë°©í–¥ê³¼ (-) ë°©í–¥ìœ¼ë¡œ í©ì–´ì§„ ë§¤ì¶œì˜ ì°¨ì´ê°€ ìƒì‡„ë˜ê¸° ë•Œë¬¸ì´ë‹¤.
  * ë”°ë¼ì„œ, í¸ì°¨ì˜ í•©ìœ¼ë¡œëŠ” ë§¤ì¶œì˜ í©ì–´ì§„ ì •ë„ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ë‹¤.

* ë¶„ì‚°(Definition of Variance)

  * ì–´ë–»ê²Œ í•˜ë©´ ìƒì‡„ë˜ëŠ” ì˜í–¥ë ¥ì„ ì—†ì•¨ ìˆ˜ ìˆì„ê¹Œ?
  * (+), (-) ë¶€í˜¸ë¥¼ ì—†ì• ê¸° ìœ„í•´ í¸ì°¨ì˜ ì œê³±ì„ ì´ìš©í•˜ê¸°ë¡œ í•œë‹¤.

  $$
  Nê°œì˜\ í™•ë¥ ë³€ìˆ˜ê°€\ ê°ê°\ x_1,x_2,x_3,...,x_nì´ë¼ëŠ”\ ê°’ì„\ ê°€ì§ˆ\ ë•Œ\ í‰ê· \ ê°’ì´\ \bar{x} ì¼\ ë•Œ\ ë¶„ì‚°ì€\ ë‹¤ìŒê³¼\ ê°™ë‹¤. \\
  \sigma^2=\frac{1}{n}\sum_{k=1}^n (x_k-\bar{x})^2
  $$

* ë¶„ì‚° vs í‘œì¤€í¸ì°¨ (Variance vs Standard Deviation)
  * ë¶„ì‚°ì˜ ë¬¸ì œ
    * ì œê³±í•œ ê°’ì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ë‹¨ìœ„ë¥¼ í‘œí˜„í•˜ê¸°ê°€ ì• ë§¤í•´ ì§
    * ë”°ë¼ì„œ, ë‹¨ìœ„ì˜ ë¬¼ë¦¬ì  ì˜ë¯¸ë¥¼ ì°¾ê¸° ìœ„í•´ ë¶„ì‚°ì˜ ì œê³±ê·¼ì¸ í‘œì¤€í¸ì°¨(standard deviation)ë¥¼ ì‚¬ìš©í•˜ê¸°ë¡œ ì•½ì†í•¨

$$
Nê°œì˜\ í™•ë¥ ë³€ìˆ˜ê°€\ ê°ê°\ x_1,x_2,x_3,...,x_n\ ì´ë¼ëŠ”\ ê°’ì„\ ê°€ì§ˆ\ ë•Œ\ í‰ê· \ ê°’ì´\ \bar{x}ì¼\ ë•Œ\ í‘œì¤€í¸ì°¨ëŠ”\ ë‹¤ìŒê³¼\ ê°™ë‹¤. \\
\sigma = \sqrt{\sigma^2} = \sqrt{\frac{1}{n}\sum_{k=1}^n (x_k-\bar{x})^2}
$$

```
Question) ì„¸ ëª…ì˜ ê³ ê° ì¤‘ì—ì„œ ì „ì²´ ë§¤ì¶œì˜ ì›”ê°„ ë™í–¥ì— ë°˜ì‘í•˜ë©° íŠ¸ë Œë“œì— ë¯¼ê°í•œ êµ¬ë§¤ ì„±í–¥ì„ ë³´ì´ëŠ” ê³ ê°ì€ ëˆ„êµ¬ì¼ê¹Œ?
```

```
Answer) ì„¸ëª…ì˜ ê³ ê° ê°ê°ì´ ì›” ë§¤ì¶œê³¼ ì–´ëŠ ì •ë„ì˜ ìƒê´€ê´€ê³„(Covariance)ë¥¼ ê°€ì¡ŒëŠ”ê°€ë¥¼ ê³ ë¯¼í•˜ë©´ ëœë‹¤.
```

$$
ë‘\ ê°€ì§€\ ë°ì´í„°ì—\ ëŒ€í•œ\ nì¡°ì˜\ í™•ë¥ ë³€ìˆ˜\ (X,Y)={(x1,y1),(x2,y2),...,(xn,yn)}ì´\ ìˆë‹¤ê³ \ ê°€ì •í•œë‹¤.\\ \ xì˜\ í‰ê· ì´\ \mu_x ì´ê³ \ Yì˜\ í‰ê· ì´\ \mu_y ë¼ê³ \ í• \ ë•Œ\ ê³µë¶„ì‚°\ Cov(X,Y)\ ëŠ”\ ë‹¤ìŒê³¼\ ê°™ë‹¤.
$$

$$
Cov(X,Y) = \frac{1}{n}\sum_{k=1}^n(x_k-\mu_x)(y_k-\mu_y)
$$

* ê³µë¶„ì‚°(Covariance)
  * ê³µë¶„ì‚° > 0 : ì¦ê°€í•˜ëŠ” ê´€ê³„ì— ìˆë‹¤.
  * ê³µë¶„ì‚° < 0 : ê°ì†Œí•˜ëŠ” ê´€ê³„ì— ìˆë‹¤.
  * ê³µë¶„ì‚° = 0 : ìƒê´€ê´€ê³„ê°€ ì—†ìœ¼ë©°, ì„œë¡œ ë…ë¦½ì´ë‹¤.
* ê³µë¶„ì‚° vs ìƒê´€ê³„ìˆ˜(Covariance vs Correlation Coefficient)
  * ê³µë¶„ì‚°ì˜ ë¬¸ì œ
    * ì œê³±í•œ ê°’ì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ë‹¨ìœ„ë¥¼ í‘œí˜„í•˜ê¸°ê°€ ì• ë§¤í•´ ì§
    * ë”°ë¼ì„œ ë‹¨ìœ„ì˜ ë¬¼ë¦¬ì  ì˜ë¯¸ë¥¼ ì°¾ê¸° ìœ„í•´ ê³µë¶„ì‚°ì„ ê°ê°ì˜ í‘œì¤€í¸ì°¨ë¡œ ë‚˜ëˆ„ì–´ ë‹¨ìœ„ë¥¼ ì—†ì• ë²„ë¦° ê°’ ìƒê´€ê³„ìˆ˜(correlation coefficient)ë¥¼ ì‚¬ìš©í•˜ê¸°ë¡œ í•¨.
    * ì´ ê³¼ì •ì—ì„œ ìƒê´€ê³„ìˆ˜ëŠ” -1 ~ 1 ì‚¬ì´ ê°’ì„ ê°€ì§€ê²Œ ë˜ê³ , ì´ëŸ¬í•œ ê³¼ì •ì„ ì •ê·œí™”ë¼ê³  í•œë‹¤.

$$
í™•ë¥ ë³€ìˆ˜\ Xì™€\ Yì˜\ ë¶„ì‚°ì´\ ì–‘ìˆ˜ì´ê³ ,\ ê°ê°ì˜\ í‘œì¤€í¸ì°¨ê°€\ \sigma_X,\sigma_Y,\\ ê³µë¶„ì‚°ì´\ \sigma_{XY}\ ë¼ê³ \ í• \ ë•Œì˜\ ìƒê´€ê³„ìˆ˜ëŠ”\ ë‹¤ìŒê³¼\ ê°™ë‹¤.\ (ì´ë•Œ,\ -1\le \rho \le 1 )\\
\rho = \frac{\sigma_{XY}}{\sigma_X\sigma_Y}
$$

* ìƒê´€ê³„ìˆ˜(Correlation Coefficient)

  * ì‹¤ì œë¡œ $$\rho_{RA}\ \rho_{RP}$$ ë¥¼ ê³„ì‚°í•´ ë´…ì‹œë‹¤.

  * | ê³ ê°ëª… | 1ì›”        | 2ì›”     | 3ì›”     | 4ì›”        | 5ì›”        | 6ì›”        | ì†Œê³„     |
    | ------ | ---------- | ------- | ------- | ---------- | ---------- | ---------- | -------- |
    | ë°±ì†Œì—° | 5,000ì›    | 5,000ì› | 5,000ì› | 5,000ì›    | 5,000ì›    | 5,000ì›    | 30,000ì› |
    | ì´ë¯¼ì¤€ | 10,,000ì›  | 3,000ì› | 1,000ì› | 1,000ì›    | 15,000ì›   | 0ì›        | 30,000ì› |
    | ì´ìš©ì§„ | 3,000ì›    | 7,000ì› | 2.000ì› | 8.000ì›    | 4.000ì›    | 6.000ì›    | 30,000ì› |
    | ...    | ...        | ...     | ...     | ...        | ...        | ...        | ...      |
    | ì›”ë§¤ì¶œ | 2ì²œ5ë°±ë§Œì› | 4ì²œë§Œì› | 2ì²œë§Œì› | 5ì²œ5ë°±ë§Œì› | 3ì²œ5ë°±ë§Œì› | 4ì²œ5ë°±ë§Œì› | 2ì–µ2ì²œë§Œ |



* Maximum Likelihood Estimation(MAP)

  * Maximum Likelihood Estimation(ìµœëŒ€ìš°ë„ì¶”ì •)ì´ë€?
    * íŒŒë¼ë§ˆí„°($$\thetaâ€‹$$)ì— ëŒ€í•œ ê°€ëŠ¥ë„í•¨ìˆ˜ $$\mathcal{L}(\theta) â€‹$$ ë¥¼ ìµœëŒ€í™” í•  ìˆ˜ ìˆëŠ” $$\theta â€‹$$ ê°’ì„ êµ¬í•˜ëŠ” ê²ƒ
    * ì¦‰, 1ì°¨ ë¯¸ë¶„ ê°’ì´ 0ì´ ë˜ëŠ” $$\theta$$ ë¥¼ êµ¬í•˜ëŠ” ê²ƒ!

  $$
  \frac{dL(\theta)}{d\theta}=0
  $$
  * ì˜ˆ1) ì˜ˆë¥¼ ë“¤ì–´, ì–´ë–¤ ë™ì „ì„ ë˜ì ¸ì„œ ë‚˜ì˜¤ëŠ” ê²°ê³¼ê°€ í™•ë¥  ë³€ìˆ˜$$X$$ë¼ê³  í•œë‹¤ë©´, ì´ ë³€ìˆ˜ëŠ” ì•($$\uparrow$$)ê³¼ ë’¤($$\downarrow$$) ì˜ ë‘ ê°’ì„ ê°€ì§ˆ ìˆ˜ ìˆë‹¤. ë™ì „ì„ ë˜ì ¸ ì•ì´ ë‚˜ì˜¬ í™•ë¥ ì´

  * $$
    Pr(X=\uparrow)=\theta
    $$

    ë¡œ ì£¼ì–´ì§€ëŠ” ê²½ìš°, ë™ì „ì„ ì„¸ ë²ˆ ë˜ì ¸ ì•, ë’¤, ì•ì´ ë‚˜ì™”ì„ ë•Œì˜ $$\theta$$ ì˜ ê°€ëŠ¥ë„ëŠ” 
    $$
    \mathcal{L}(\theta|\uparrow\downarrow\uparrow)=\theta\ \cdot\ (1-\theta)\ \cdot\ \theta = \theta^2(1-\theta)
    $$
    ê°€ ëœë‹¤. ê°€ëŠ¥ë„ í•¨ìˆ˜ë¥¼ ì ë¶„í•˜ë©´
    $$
    \int_{0}^{1} \mathcal{L}(\theta|\uparrow\downarrow\uparrow)\, d\theta = \int_{0}^{1} \theta^2(1-\theta)\, d\theta = 1/12
    $$
    ì´ë¯€ë¡œ, ê°€ëŠ¥ë„ëŠ” í™•ë¥  ë¶„í¬ê°€ ì•„ë‹˜ì„ ì•Œ ìˆ˜ ìˆë‹¤.

* Maximum Likelihood Estimationì˜ ë¬¸ì œì ?

  * ê°€ëŠ¥ë„ìˆ˜ì˜ ì°¨ìˆ˜ê°€ ê³ ì°¨ì› ë°©ì •ì‹(ex) 100ì°¨) ì´ë¼ `ë¯¸ë¶„`ì´ ìƒë‹¹íˆ ë³µì¡!!

*  **ë¡œê·¸** ê°€ëŠ¥ë„í•¨ìˆ˜ ë„ì…

  * $$log_eL(\theta)$$
  * ë¡œê·¸ë¥¼ ì‚¬ìš©í•˜ë©´, ê³±ì…ˆì„ ë§ì…ˆìœ¼ë¡œ ë°”ê¿€ ìˆ˜ ìˆê³ 
  * ë¡œê·¸ë¥¼ ì‚¬ìš©í•˜ë©´, 100ì°¨ ë°©ì •ì‹ì„ 1ì°¨ë°©ì •ì‹ìœ¼ë¡œ ëª¨ì–‘ì„ ë°”ê¾¸ì–´ ë¯¸ë¶„ ê°€ëŠ¥

  $$
  \frac{d}{d\theta}log_eL(\theta)=0
  $$

  

* ì‹¤ì œë¡œ ê³¼ê±°ì˜ ë°ì´í„°ë¡œë¶€í„° ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•  ë•Œ ì´ëŸ¬í•œ ë°©ë²•ì„ ì‚¬ìš©í•¨

$$
L(\theta_1,\theta_2,....,\theta_m)ì„\ ìµœëŒ€ë¡œ\ í•˜ëŠ”\ \theta_1,\theta_2,...,\theta_mì€\ ë‹¤ìŒ\ ë°©ì •ì‹ì„\ ë§Œì¡±í•œë‹¤. \\
\frac{d}{d\theta_1}L(\theta_1,\theta_2,....,\theta_m)=0 \\
\frac{d}{d\theta_2}L(\theta_1,\theta_2,....,\theta_m)=0 \\
... \\
\frac{d}{d\theta_m}L(\theta_1,\theta_2,....,\theta_m)=0 \\
$$



### ê°•ì˜ 5.

### $ìˆ˜\ ì¹˜\ í•´\ ì„\  :\ =\ ìˆ˜\ ì¹˜\ ê³„\ ì‚°\ $ 

* ìˆ˜ì¹˜í•´ì„ì´ë€? 
  * ìˆ˜ì¹˜ì ìœ¼ë¡œ ê·¼ì‚¬ê°’ì„ êµ¬í•˜ëŠ” ë°©ë²•ì„ ì—°êµ¬í•˜ëŠ” ë¶„ì•¼
  * ì„ í˜•ë°©ì •ì‹ì˜ í•´, ë³´ê°„ë²•, ë¯¸ë¶„ë°©ì •ì‹, ì ë¶„ë°©ì •ì‹, ê³ ìœ ì¹˜ë¬¸ì œ, ìœ í•œìš”ì†Œë²•, ìµœì í™” ì´ë¡ 
* ë”¥ëŸ¬ë‹ì„ ìœ„í•œ ìµœì í™”
  * ë”¥ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì—ëŠ” ìµœì í™”ê°€ í•„ìˆ˜
  * ìµœì í™”ë€ xê°’ì„ ë°”ê¿”ê°€ë©´ì„œ f(x)ì˜ ê°’ì„ ìµœëŒ€/ìµœì†Œí™” í•˜ëŠ” ê²ƒ
  * ëŒ€ë¶€ë¶„ ìµœì†Œê°’ f(x) ë¥¼ ì°¾ìŒ
  * ìµœëŒ€í™”ëŠ” -f(x)ë¥¼ ìˆ˜í–‰í•˜ë©´ ë¨
  * ì¼ë°˜ì ìœ¼ë¡œ f(x)ëŠ” Objective function(ëª©ì í•¨ìˆ˜), criterion(íŒì •ê¸°ì¤€)ì´ë¼ê³  ë¶ˆë¦¼
  * ìµœì†Œê°’ì„ ì°¾ëŠ” f(x)ì˜ ê²½ìš° Cost function(ë¹„ìš©í•¨ìˆ˜), Loss function(ì†ì‹¤í•¨ìˆ˜), Error function(ì˜¤ì°¨í•¨ìˆ˜)

* ë”¥ëŸ¬ë‹ì„ ìœ„í•œ ìˆ˜ì¹˜ê³„ì‚°

  * ìˆ˜í•™ ê³µì‹ì„ í•´ì„ì ìœ¼ë¡œ ìœ ë„í•´ì„œ í•´ê²°í•˜ì§€ ì•ŠìŒ

  * ë°˜ë³µì ì¸ ê³¼ì •ì„ í†µí•´ì„œ ì •ë‹µì˜ ì¶”ì •ê°’ì„ ê³„ì† ê°±ì‹ í•˜ì—¬ ë¬¸ì œë¥¼ í’ˆ

    ì˜ˆ) ìµœì†Œ, ìµœëŒ€ë¥¼ êµ¬í•˜ëŠ” ìµœì í™” ë¬¸ì œ

  * ìˆ˜ì¹˜ê³„ì‚°ì„ ìœ„í•œ ëª‡ê°€ì§€ ì£¼ì˜ì‚¬í•­

    * ì•Œê³ ë¦¬ì¦˜ì€ 'ì‹¤ìˆ˜' í•¨ìˆ˜ë¡œ êµ¬ì„±ë˜ê¸°ë„ í•œë‹¤.
    * ì‹¤ìˆ˜ëŠ” ì»´í“¨í„°ë¡œ í‘œí˜„í•˜ëŠ”ë° 'í•œê³„'ë¥¼ ê°€ì§€ê³  ìˆë‹¤.
    * ì…ë ¥ê°’ì˜ ì‘ì€ ë³€í™”ê°€ ì¶œë ¥ê°’ì˜ í° ë³€í™”ë¥¼ ê°€ì ¸ì˜¬ ë¬¸ì œê°€ ë¨
    * ì…ë ¥ì˜ ë°˜ì˜¬ë¦¼ ì˜¤ì°¨ê°€ ì¦í­ë˜ì–´ì„œ ì¶œë ¥ì˜ ì°¨ì´ê°€ ì»¤ì§

  * Conditioning(ì¡°ê±´í™”)

    * ì…ë ¥ì˜ ì‘ì€ ë³€í™”ì— ëŒ€í•´ í•¨ìˆ˜ê°€ ì–¼ë§ˆë‚˜ ê¸‰í•˜ê²Œ ë³€í•˜ëŠ”ì§€ë¥¼ ëœ»í•˜ëŠ” ìš©ì–´(ì§„ë™)

  * Condition Number(ì¡°ê±´ìˆ˜)

    * ê°€ì¥ í° ê³ ìœ³ê°’ê³¼ ê°€ì¥ ì‘ì€ ê³ ìœ³ê°’ì˜ í¬ê¸°(ì ˆëŒ€ê°’)ì˜ ë¹„
    * ì´ ë¹„ê°€ í¬ë©´, ì—­í–‰ë ¬ ê³„ì‚°ì€ ì…ë ¥ì˜ ì˜¤ì°¨ì— íŠ¹íˆë‚˜ ë¯¼ê°í•¨

    $$
    Condition\ Number = max_{i,j}	\left| \frac{\lambda_i}{\lambda_j} \right|
    $$

* Roadmap

  * Iterative Optimization
  * Rounding error, underflow, overflow

* ì¼ì°¨ ë¯¸ë¶„ì˜ ê°€ì¥ ë‹¨ìˆœí•œ í˜•íƒœëŠ” ìˆ˜ì‹ì— ë”°ë¼ ì„ì˜ì˜ ì‹œì‘ì ìœ¼ë¡œë¶€í„° ìˆ˜ë ´í•  ë•Œê¹Œì§€ xë¥¼ ë³€í™”ì‹œí‚¤ëŠ” ê²ƒ
  $$
  X_{k+1} = X_k - \lambda f'(X_k)
  $$

* Iterative Optimization : Curvature

  ì´ì°¨ ë¯¸ë¶„ì˜ ê°€ì¥ ë‹¨ìˆœí•œ í˜•íƒœëŠ” 
  $$
  X_{k+1} = X_k-\frac{f'(X_k)}{f''(X_k)}
  $$
  ìµœì í™” ê¸°ë²• : LM ê³µë¶€

* Approximate Optimization

  * global minimum(ì „ì—­ ìµœì†Œì )
  * local minimum(êµ­ì†Œ ìµœì†Œì )

![Ã¬Â‚Â°Ã«Â‚Â´Ã« Â¤Ã¬Â˜Â¤Ã«ÂŠÂ”Ã¬ÂÂ‘Ã¬ÂÂ€Ã¬Â˜Â¤Ã¬Â†Â”ÃªÂ¸Â¸Ã¬ÂÂ˜Ã¬Â°Â¾ÃªÂ¸Â°(Optimizer)Ã¬ÂÂ˜Ã«Â°ÂœÃ«Â‹Â¬ÃªÂ³Â„Ã«Â³Â´ SGD Momentum NAG Adagrad RMSProp AdaDelta Adam Nadam Ã¬ÂŠÂ¤Ã­Â…ÂÃªÂ³Â„Ã¬Â‚Â°Ã­Â•Â´Ã¬Â„ÂœÃ¬Â›Â€Ã¬Â§ÂÃ¬ÂÂ¸Ã­Â›Â„,Ã¢Â€Â¨ Ã¬Â•Â„ÃªÂ¹ÂŒÃ«Â‚Â´Ã« Â¤Ã¬Â˜Â¤Ã«ÂÂ˜ÃªÂ´Â€Ã¬Â„Â±Ã«Â°Â©Ã­Â–Â¥Ã«Â˜ÂÃªÂ°Â€Ã¬ÂÂ Ã¬ÂÂ¼Ã«Â‹Â¨ÃªÂ´Â€Ã¬Â„Â±Ã«Â°Â©Ã­Â–Â¥Ã«Â¨Â¼Ã¬ Â€Ã¬Â›Â€Ã¬Â§ÂÃ¬ÂÂ´ÃªÂ³ ...](https://image.slidesharecdn.com/random-170910154045/95/-49-638.jpg?cb=1505089848)

* Critical Points
  * $$f'(x) = 0$$ ì¸ ì 
  * Local minimum point(ê·¹ì†Œì ), Local maximum point(ê·¹ëŒ€ì ), saddle point(ì•ˆì¥ì  : ê·¹ì†Œë„ ê·¹ëŒ€ë„ ì•„ë‹Œ ì )

* ë‹¤ë³€ìˆ˜ì—ì„œëŠ” ëª¨ë“  í¸ë¯¸ë¶„ì´ 0ì¸ ì§€ì ì„ ì°¾ì•„ì•¼í•œë‹¤.
* ê·¸ ì‹ì´ Jacobian(ì•¼ì½”ë¹„ì–¸) í–‰ë ¬ê³¼ Hessian(í—¤ì‹œì•ˆ) í–‰ë ¬ì´ë‹¤



### ê°•ì˜ 6.

* ì‘ì§€ë§Œ ì‹œì¥ì´ ê°œì²™ë˜ì–´ì•¼ë§Œ ê¸°ìˆ ì´ ë°œì „í•œë‹¤

  * Uber : 10ì–µ ë¹„ìš©ì˜ ììœ¨ì£¼í–‰ì°¨ëŸ‰ì„ ì œì‘í•´ì„œ ì‹œì¥ì„ ê°œì²™í•˜ê³ ì í•œë‹¤.

* ë¨¸ì‹ ëŸ¬ë‹  : ë°ì´í„°(ê³¼ì¼ë“¤) + ì •ë‹µ(ìš”ë¦¬) => í•¨ìˆ˜(ë ˆì‹œí”¼)ë¥¼ ê¸°ê³„ê°€ í•™ìŠµí•˜ëŠ” ë°©ë²•

* ê°€ì„¤ ì„¤ì • : $$H(x) = Wx+bâ€‹$$  

* ë¹„ìš© í•¨ìˆ˜ (Cost function) : $$H(x) - y$$ 
  $$
  cost = \frac{1}{m}\sum_{i=1}^m(H(x^{(i)})-y^{(i)})^2
  $$

$$
cost(W,b) = \frac{1}{m}\sum_{i=1}^m(H(x^{(i)})-y^{(i)})^2
$$

* ëª©í‘œ : $$minimize\ cost(W,b)$$  
* costë¥¼ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•˜ëŠ” ë°©ë²•?  => Optimization Algorithmì„ ì‚¬ìš©í•œë‹¤
* ![Ã¬Â‚Â°Ã«Â‚Â´Ã« Â¤Ã¬Â˜Â¤Ã«ÂŠÂ”Ã¬ÂÂ‘Ã¬ÂÂ€Ã¬Â˜Â¤Ã¬Â†Â”ÃªÂ¸Â¸Ã¬ÂÂ˜Ã¬Â°Â¾ÃªÂ¸Â°(Optimizer)Ã¬ÂÂ˜Ã«Â°ÂœÃ«Â‹Â¬ÃªÂ³Â„Ã«Â³Â´ SGD Momentum NAG Adagrad RMSProp AdaDelta Adam Nadam Ã¬ÂŠÂ¤Ã­Â…ÂÃªÂ³Â„Ã¬Â‚Â°Ã­Â•Â´Ã¬Â„ÂœÃ¬Â›Â€Ã¬Â§ÂÃ¬ÂÂ¸Ã­Â›Â„,Ã¢Â€Â¨ Ã¬Â•Â„ÃªÂ¹ÂŒÃ«Â‚Â´Ã« Â¤Ã¬Â˜Â¤Ã«ÂÂ˜ÃªÂ´Â€Ã¬Â„Â±Ã«Â°Â©Ã­Â–Â¥Ã«Â˜ÂÃªÂ°Â€Ã¬ÂÂ Ã¬ÂÂ¼Ã«Â‹Â¨ÃªÂ´Â€Ã¬Â„Â±Ã«Â°Â©Ã­Â–Â¥Ã«Â¨Â¼Ã¬ Â€Ã¬Â›Â€Ã¬Â§ÂÃ¬ÂÂ´ÃªÂ³ ...](https://image.slidesharecdn.com/random-170910154045/95/-49-638.jpg?cb=1505089848)

* Gradient descent algorithm(ê²½ì‚¬ í•˜ê°• ì•Œê³ ë¦¬ì¦˜)

  * cost function ìµœì†Œí™”

  * ë§ì€ ìµœì†Œí™” ë¬¸ì œì— ì‚¬ìš©í•œë‹¤

  * ì£¼ì–´ì§„ $$cost(W,b)$$ ì—ì„œ $$cost$$ë¥¼ ìµœì†Œí™”í•˜ëŠ” $$W,b$$ ë¥¼ ì°¾ëŠ” ê²ƒ

  * $$cost(w_1,w_2,w_3...,b)$$ ë„ ê°€ëŠ¥í•˜ë‹¤

  * Gradient Descent Algorithm

  * $$
    W :=W - \alpha\frac{\partial}{\partial W}cost(W)
    $$

    

* Convex Function(Global minimumì„ ë³´ì¥í•˜ëŠ”ì§€ ê²°ì •ì§“ëŠ” í•¨ìˆ˜)

* $$
  cost(W,b) = \frac{1}{m}\sum_{i=1}^m(H(x^{(i)})-y^{(i)})^2
  $$

* ![ÃªÂ´Â€Ã« Â¨ Ã¬ÂÂ´Ã«Â¯Â¸Ã¬Â§Â€](https://lh5.googleusercontent.com/DLRjNnXFnpUMKd2FjxX3fTCjLIjd_NRFFxfZB4jzPZmTbSiDWXQ-D9JNgcHrBOTTCGtcWtmNmPEpY3MX0BbOjoMgSg0CubWHnzp-mf7FBWqftlMQewNKu6iOsuF9_DgUcnD3AyEC)

  ![convexÃ¬Â—Â Ã«ÂŒÂ€Ã­Â•Âœ Ã¬ÂÂ´Ã«Â¯Â¸Ã¬Â§Â€ ÃªÂ²Â€Ã¬ÂƒÂ‰ÃªÂ²Â°ÃªÂ³Â¼](https://undergroundmathematics.org/glossary/convex-shape/images/convex.png)



* Linear Regression(ê¸°ì¶œí•˜ì‹ ë‹¤ê³ í•¨)
  * Hypothesis
    * $$H(X) = WX$$
  * Cost function
    * $$cost(W) = \frac{1}{m}\sum{(WX-y)^2}$$
  * Gradient descent algorithm
    * $$W :=W - \alpha \frac{\}{}$$




### ê°•ì˜ 7.

* ì–´ë–»ê²Œ linear regressionì„ ì‚¬ìš©í•´ì„œ ì´ì§„ ë¶„ë¥˜ë¥¼ í•˜ëŠ”ê°€

* ì˜¤ëŠ˜ì˜ ëª©í‘œ : ì´ì§„ ë¶„ë¥˜(binary classification)

  * example) Spam Detection, Facebook feed, Credit Card Fraudulent Transaction detection

* Binary Label Encoding -> '0' or '1'

  * Spam Detection : Spam(1) or Ham(0)
  * Facebook feed : show(1) or hide(0)
  * Credit Card Fraudulent Transaction detection: legitimate(1)/ fraud(0)

* ì´ì§„ ë¶„ë¥˜ì˜ ì˜ˆ

  * Positive or Negative
  * Buy or Sell
  * Pass or Fail

* Linear Regressionì´ ì´ì§„ ë¶„ë¥˜ì—ì„œ ë‹¨ì ì€?

  * ë‹¨ì  1 : ì„ í˜•ìœ¼ë¡œëŠ” 0ê³¼ 1ì„ ì œëŒ€ë¡œ ê°€ë¥´ì§€ ëª»í•œë‹¤. íŠ¹ì • ê°’ì„ ì•ˆìœ¼ë¡œ ì§‘ì–´ ë„£ëŠ”ë‹¤ -> Bound ë˜ì§€ ì•ŠëŠ”ë‹¤.

* ì´ì§„ ë¶„ë¥˜ì—ì„œ Linear Regressionì„ ì‚¬ìš©í•˜ë©´?

  * ìš°ë¦¬ëŠ” Yë¥¼ 0ê³¼ 1 ë‘ê°€ì§€ ë°ì´í„°ë¡œ ì•Œê³  ìˆë‹¤.
  * $$H(x) = Wx+b$$ 
  * ( + ) : ì´ HypothesisëŠ” ê°„ë‹¨í•˜ê³  ì‚¬ìš©í•˜ê¸° ì‰½ë‹¤.
  *  ( - ) : ì´ hypothesisëŠ” 1ë³´ë‹¤ í¬ê±°ë‚˜ 0ë³´ë‹¤ ì‘ì€ ê°’ì„ ì¤„ ìˆ˜ ìˆë‹¤.ë„ˆë¬´ í¬ê³  ë„ˆë¬´ ì‘ì€ ê°’ì´ ë‚˜ì˜¨ë‹¤. 0ê³¼ 1ì‚¬ì´ì˜ ê°’ì´ ì•ˆë‚˜ì˜¨ë‹¤.

* Logistic Hypothesis

  * $$H(x) = Wx+b$$ ì˜ ì‹ì„ ì–´ë–»ê²Œ 0ê³¼ 1ì‚¬ì´ë¡œ ë°”ìš´ë“œ ì‹œí‚¬ê¹Œ?

  * 0 <= H(x) <= 1 

  * Sigmoid : ë‘ ë°©í–¥ì—ì„œ ì»¤ë¸Œê°€ ìˆëŠ” í•¨ìˆ˜ë‹¤.

  * $$Logistic\  function := sigmoid\ function$$

  * Sigmoid í•¨ìˆ˜ ë•ë¶„ì— H(x)ê°€ Bound ë˜ì—ˆë‹¤.
    $$
    H(X) = \frac{1}{1+e^{-(W^TX)}}
    $$

    $$
    0<=H(x)<=1
    $$

    

* Cost function

  * ì´ì œ Cost function ì— ì ìš©í•´ë³´ì.

  * linear regression cost functionì— ì ìš©í•˜ë‹ˆ, local minimumì— ë¹ ì§„ë‹¤.

  * $$
    H(x) = Wx + b \Rightarrow Convex function
    $$

    $$
    H(X) = \frac{1}{1+e^{-W^TX}} \Rightarrow Non-Convex function
    $$

    ```
    ì„¤ê³„ íŒ : 
    ì •ë‹µì— ê°€ê¹Œì›Œ ì§ˆìˆ˜ë¡ Cost function ê°’ì€ ì‘ê³ 
    ì •ë‹µì—ì„œ ë©€ì–´ì§ˆ ìˆ˜ë¡ Cost function ê°’ì€ í¬ê²Œ ! 
    ì„¤ê³„í•˜ë©´ ëœë‹¤.
    ```

* Cost function for logistic regression

  * Logistic regression function

  * $$
    H(X) = \frac{1}{1+e^{-W^TX}}
    $$

  * 

  $$
  cost(W) = \frac{1}{m}\sum \ \ \ \ \ c(H(x),y)
  $$

  * y=1ì´ trueì¼ ë•Œì™€ y=0ì´ trueì¼ ë•Œì˜ í•¨ìˆ˜

  $$
  c(H(x),y) = \begin{cases}
  -log(H(x)) & :y=1 \\
  -log(1-H(x)) & :y=0
  \end{cases}
  $$

  * í•˜ë‚˜ì˜ equationìœ¼ë¡œ ì˜ í‘œí˜„í•˜ë©´
    $$
    C(H(x),y)=-ylog(H(x))-(1-y)log(1-H(x))
    $$

* Gradient decent algorith

$$
W:=W-\alpha\frac{\partial}{\partial W}cost(W)
$$

```python
cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(hypothesis) + (1-y)* tf.log(1-hypothesis)))

# Minimize
a = tf.Variable(0.1) # Learning rate, alpha
optimizer = tf.train.GradientDescentOptimizer(a)
train = optimizer.minimize(cost)
```

* Binary Classificationì„ ìœ„í•œ Logistic regression 
  1. $$H_L(x) = WXâ€‹$$
  2. $$z = H_L(x), \ g(z)â€‹$$
  3. $$g(z) = \frac{1}{1+e^{-2}}â€‹$$
  4. $$H_R(x) = g(H_L(x))$$ 



* Multi-Class classificationì„ Binary classificationìœ¼ë¡œ ì–´ë–»ê²Œ í• ê¹Œ?

* Multinomial classification(ì—¬ëŸ¬ ê°œì˜ í´ë˜ìŠ¤!)
* A or not / B or not / C or not ìœ¼ë¡œ ë‚˜ëˆˆë‹¤~
* Softmax?
  * Multiclass classificationì˜ Hypothesisë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ [0,1]ë¡œ ì œí•œí•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë³´ì.
* Softmax function
  * ëª¨ë“  ê°’ì´ 0~1ì‚¬ì´
  * ì „ì²´ í•©ì´ 1(í™•ë¥  ì •ê·œí™”!!)
  * Probabilityì™€ ë¹„ìŠ·í•˜ë‹¤.
* Cross-entropy



### ê°•ì˜ 8.

* í”„ë¡œì íŠ¸ 4/29ê¹Œì§€ ì œì•ˆì„œ ì œì¶œ

  1. ì£¼ì œ ì„ ì • ì´ìœ 
  2. íŒ€ì› ì—­í•  ë¶„ë‹´
  3. ë°ì´í„°ì…‹
  4. í˜„ì¬ê¹Œì§€ì˜ ì—°êµ¬ì„±ê³¼

* ì˜ì œ

  1. Learning Rate
  2. Data preprocessing
  3. Avoid overfitting
     1. ë” ë§ì€ ë°ì´í„°
     2. ê·œì œ
  4. Performance evaluation

* Learning rate : í•™ìŠµë¥ 

  * Loss function
  * Large learning rate : overshooting
  * Small learning rate : Takes too long
  * cost functionì„ ê´€ì¢‹ì€ í•™ìŠµë¥ ì„ ì²´í¬í•˜ê¸°

* Data Preprocessing : ë°ì´í„° ì „ì²˜ë¦¬

  * ë°ì´í„°ì˜ ì»¬ëŸ¼ ë³„ë¡œ ë‹¤ë¥¸ ë¹„ìš©ì„ ì§€ë¶ˆí•œë‹¤ : Normalize
  * ë°ì´í„°ë¥¼ ì •ê·œí™”í•˜ì§€ ì•Šìœ¼ë©´, ì°¾ì•„ê°€ë©´ì„œ ë¹„ìš©ì´ ë°œì‚°í•  ê°€ëŠ¥ì„±ì´ ìˆë‹¤.
  * ë°ì´í„° ì •ê·œí™” : origin -> zero-centered data -> normalized data

* Overfitting

  * ë°ì´í„°ì— ë…¸ì´ì¦ˆê°€ ë‚Œ.. ê·¸ëŸ°ë° ê³¼ì í•©ì´ ëœ ëª¨ë¸ì€ ê·¸ê²ƒë§ˆì €ë„ í•™ìŠµì„ í•˜ëŠ” í˜„ìƒì„ ë³´ì„

* Overfitting Solution : Regularization

  * í•™ìŠµ ë°ì´í„°ë¥¼ ë§ì´ ëª¨ìŒ

  * ê·œì œ

    * L2Regularization : $$cost = \frac{1}{N}\sum D(S(WX_i+b),L_i)+ \lambda\sum W^2$$

    * ëŒë‹¤ ê°’ì€ ë§¤ë‰´ì–¼í•˜ê²Œ ë°”ê¿”ê°€ë©´ì„œ ì •í•´ê°€ë©´ ë¨.

    * ```
      L2reg = 0.001*tf.reduce_sum(tf.square(W))
      ```

    * L1Regularization = $$cost = \frac{1}{N}\sum D(S(WX_i +b ),L_i)+ \lambda\sum |W|$$

* Performance evaluation

  * í•™ìŠµì— ì‚¬ìš©í•œ ë°ì´í„°
  * í•™ìŠµì— ì‚¬ìš©í•˜ì§€ ì•Šì€ ë°ì´í„°
  * Training, Testing ë¶„ë¦¬
  * Trainingì—ì„œ Validationì„ ë–¼ì–´ë‚´ë¼
  * Validation ì¤‘ìš”í•˜ë‹¤ ë¬´ì¡°ê±´ ë‚˜ëˆ ë¼!

* MNIST Dataset

* MLP : Multi Layer Perceptron

* Feed Forward , Backpropagation

* Gradient Descent ëŠ” Feed Forwardì´ê³  ì—­ì „íŒŒëŠ” ë‹¤ë¥¸ ê°œë…??!



### ê°•ì˜ 9.

* ë°°ë‹¬ì˜ ë¯¼ì¡± 'ììœ¨ì£¼í–‰ ë°°ë‹¬ë¡œë´‡'

* Marvin Minsky êµìˆ˜ê°€ ë§í•˜ê¸¸, 'ì—¬ëŸ¬ ì¸µì„ ìŒ“ì•„ ë” ë³µì¡í•œ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆë‹¤.'

* NNì„ ì‚¬ìš©í•´ì„œ XORë¬¸ì œë¥¼ í•´ê²°í•œë‹¤.

  ![image-20190416121457117](/Users/donghoon/Library/Application Support/typora-user-images/image-20190416121457117.png)

* ì—¬ëŸ¬ ì—°ë¦½ ë°©ì •ì‹ì„ approximationí•´ í•˜ë‚˜ì˜ ë ˆì´ì–´ë¡œ í•©ì¹œë‹¤.

  ![image-20190416121556194](/Users/donghoon/Library/Application Support/typora-user-images/image-20190416121556194.png)

* BackPropagation(Chain rule)

  ![image-20190416123055017](/Users/donghoon/Library/Application Support/typora-user-images/image-20190416123055017.png)

![image-20190416123801823](/Users/donghoon/Library/Application Support/typora-user-images/image-20190416123801823.png)



![image-20190416123832440](/Users/donghoon/Library/Application Support/typora-user-images/image-20190416123832440.png)



* ê²°ë¡ ì ìœ¼ë¡œ, ì•ì—ì„œ ë°°ì› ë˜ Gradient Descent ì•Œê³ ë¦¬ì¦˜ì€ ê°ê°ì˜ wì™€ bê°€ lossì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¯¸ë¶„ì„ í†µí•´ êµ¬í•´ì•¼í•˜ëŠ”ë°, Backpropationìœ¼ë¡œ ì—…ë°ì´íŠ¸í•  ë•Œ Chainruleì„ ì ìš©í•˜ì—¬ ë¯¸ë¶„ê°’ì„ ì ì€ ê³„ì‚°ìœ¼ë¡œ êµ¬í•œë‹¤. $$w_t=w_{t-1}-learning\ rate*{w_t}'$$ ì˜ Gradient Descentë¥¼ ì‰½ê²Œ í• ìˆ˜ìˆê²Œ í•œë‹¤.(ë³µì¡ë„ë¥¼ ì¤„ì„)



* RELU( REctifier Linear Unit)

  * Sigmoid functionì˜ ì¶œë ¥ ê°’ì€ $$ 0<=f(input)<=1$$ ê°’ì´ë¯€ë¡œ Backpropagationì„ í•  ë•Œ, ì‰½ê²Œ Gradient Vanishing ë¬¸ì œê°€ ìƒê¸´ë‹¤.
  * ë”°ë¼ì„œ, RELU activation functionì„ í™œìš©í•˜ì—¬ ì´ ë¬¸ì œì ì„ ì–´ëŠì •ë„ í•´ì†Œí•˜ì˜€ë‹¤. -> í•™ìŠµ ì„±ëŠ¥ í–¥ìƒ

  ![image-20190416125010089](/Users/donghoon/Library/Application Support/typora-user-images/image-20190416125010089.png)

  * RELUëŠ” overshooting í˜„ìƒì´ ì˜ ìƒê¸´ë‹¤. out of range ã…œã…œ

  ![image-20190416125217670](/Users/donghoon/Library/Application Support/typora-user-images/image-20190416125217670.png)

* Activation Functionì˜ ì¢…ë¥˜ë“¤

![image-20190416125404148](/Users/donghoon/Library/Application Support/typora-user-images/image-20190416125404148.png)

* ì´ë“¤ ì¤‘, ì„±ëŠ¥ì€ Maxoutì´ ê°€ì¥ ì¢‹ì•˜ë‹¤. [Mishkin et al. 2015]

* ì œí”„ë¦¬ íŒíŠ¼ êµìˆ˜ë‹˜ì˜ ë§ì”€

  * ìš°ë¦¬ì˜ ë ˆì´ë¸” ë°ì´í„°ëŠ” ë„ˆë¬´ ì‘ë‹¤.
  * ìš°ë¦¬ì˜ ì»´í“¨í„°ëŠ” ë„ˆë¬´ ëŠë¦¬ë‹¤
  * ìš°ë¦¬ëŠ” ì˜ëª»ëœ ë°©ì‹ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì´ˆê¸°í™”í–ˆë‹¤
  * ìš°ë¦¬ëŠ” í‹€ë¦° ì„ í˜•ì„±ì˜ íƒ€ì…ì„ ì‚¬ìš©í–ˆë‹¤.

* ì—¬ê¸°ì„œ ì˜ëª»ëœ ë°©ì‹ì˜ ì´ˆê¸°í™”ì— ì§‘ì¤‘í•´ë³´ì.

* RBM(ì œí•œëœ ë³¼ì¸ ë§Œ ë¨¸ì‹ ) ëª¨ë¸

  * ê°„ë‹¨í•œ ì„¤ëª… : Structureì—ì„œ forward ë°©í–¥ìœ¼ë¡œ ë‚˜ì˜¤ëŠ” ì•„ì›ƒí’‹ê°’ì„ ê°€ì§€ê³ , ë‹¤ì‹œ Backwardë°©í–¥ìœ¼ë¡œ ê³„ì‚°í•˜ì˜€ì„ë•Œ ë‚˜ì˜¤ëŠ” ê°’ê³¼ KL Divergence ë¥¼ ê³„ì‚°í•´ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•˜ì—¬ ìœ ì‚¬í•  ìˆ˜ë¡ ì¢‹ì€ ê²ƒ!!

* RBMì´ ë„ˆë¬´ ë³µì¡í•˜ë‹ˆ, ê°„ë‹¨í•œ ë°©ë²•ìœ¼ë¡œ ì´ˆê¸°í™”í•´ë³´ì

  * Xavier Initialization : ë²¤ì§€ì˜¤ êµìˆ˜ì— ì˜í•´ 2010ë…„ì— ì œì•ˆëœ ì´ˆê¸°í™” ë°©ë²•
  * He's initialization : ì¹´ì´ë° í—ˆì— ì˜í•´ 2015ë…„ ì œì•ˆëœ ì´ˆê¸°í™” ë°©ë²•

  ```python
  # Xavier initialization
  # Gloarot et al. 2010
  W = np.random.randn(fan_in, fan_out)/np.sqrt(fan_in)
  # He et al . 2015
  W = np.random.randn(fan_in, fan_out)/np.sqrt(fan_in/2)
  ```

  * Activation functions and initialization on CIFAR-10

  ![image-20190416135910090](/Users/donghoon/Library/Application Support/typora-user-images/image-20190416135910090.png)

* ì´ë ‡ê²Œ ì´ˆê¸°í™”ì™€ ë¹„ì„ í˜• ë¬¸ì œë¥¼ í•´ê²°í–ˆë‹¤.
